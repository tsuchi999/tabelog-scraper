from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import os, time, re

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
output_dir = r"D:\tabelog"
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, "tabelog_candidates.txt")
exclude_file = os.path.join(output_dir, "exclude_names.txt")
visited_file = os.path.join(output_dir, "visited.txt")

# é™¤å¤–ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
exclude_names = set()
visited_names = set()
if os.path.exists(exclude_file):
    with open(exclude_file, encoding="utf-8") as f:
        exclude_names = set(line.strip() for line in f if line.strip())
if os.path.exists(visited_file):
    with open(visited_file, encoding="utf-8") as f:
        visited_names = set(line.strip() for line in f if line.strip())
excluded = exclude_names.union(visited_names)

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–ï¼ˆæ¯å›ä¸Šæ›¸ãï¼‰
with open(output_file, "w", encoding="utf-8") as f:
    pass

# Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
options = Options()
# options.add_argument("--headless")  # å¿…è¦ã«å¿œã˜ã¦ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹åŒ–
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option("useAutomationExtension", False)
options.add_argument("user-agent=Mozilla/5.0")

# ãƒ‰ãƒ©ã‚¤ãƒèµ·å‹•
driver = webdriver.Chrome(options=options)
driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
    "source": "Object.defineProperty(navigator, 'webdriver', { get: () => undefined })"
})

# ãƒšãƒ¼ã‚¸ã‚’5ã€œ10ã¾ã§ãƒ«ãƒ¼ãƒ—
for page in range(5, 11):
    print(f"\nğŸ“„ {page}ãƒšãƒ¼ã‚¸ç›® é–‹å§‹")
    driver.get(f"https://tabelog.com/kanagawa/rstLst/ramen/{page}/?Srt=D&SrtT=rt&sk=ãƒ©ãƒ¼ãƒ¡ãƒ³")

    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-rst__wrap"))
        )
    except Exception as e:
        print(f"âŒ èª­ã¿è¾¼ã¿NG: {e}")
        with open(os.path.join(output_dir, f"debug_page_{page}.html"), "w", encoding="utf-8") as debug_f:
            debug_f.write(driver.page_source)
        continue

    soup = BeautifulSoup(driver.page_source, "html.parser")
    cards = soup.select("div.list-rst__wrap")
    print(f"ğŸ‘‰ è¦‹ã¤ã‹ã£ãŸ: {len(cards)} åº—èˆ—")

    if len(cards) == 0:
        with open(os.path.join(output_dir, f"debug_page_{page}.html"), "w", encoding="utf-8") as debug_f:
            debug_f.write(driver.page_source)

    with open(output_file, "a", encoding="utf-8") as f:
        for card in cards:
            name_tag = card.select_one("a.list-rst__rst-name-target")
            score_tag = card.select_one("span.c-rating__val")
            rank_tag = card.select_one("span.c-ranking-badge__no")
            genre_tag = card.select_one("div.list-rst__area-genre")
            holiday_tag = card.select_one("span.list-rst__holiday-text")

            if not name_tag:
                continue

            name = name_tag.get_text(strip=True)
            url = name_tag["href"]
            if name in excluded:
                continue

            score = score_tag.get_text(strip=True) if score_tag else "?"
            rank = rank_tag.get_text(strip=True) if rank_tag else "?"
            station = ""
            if genre_tag:
                m = re.search(r"([^\s]+é§…)", genre_tag.get_text())
                station = m.group(1) if m else ""

            closed = ""
            if holiday_tag:
                closed = "ä¼‘ï¼š" + holiday_tag.get_text(strip=True)

            line1 = f"{rank}ã€€{name}ï¼ˆ{score}ï¼‰{station}"
            if closed:
                line1 += f"ã€€{closed}"

            print("â†’", line1)
            f.write(line1 + "\n")
            f.write(url + "\n\n")

    time.sleep(2)

driver.quit()
print("\nâœ… å®Œäº†ï¼å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã—ã¦ã­ã€‚")
